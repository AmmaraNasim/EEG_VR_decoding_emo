# Stereoscopic vision and emotional face processing
This code was created as part of a Master's Thesis at the Max Planck Institute for Human Cognitive and Brain Sciences and the University of Oldenburg. We combined immersive Virtual Reality (VR) technology with EEG measurements to investigate the recognition and neurophysiological responses to digital humans' emotional facial expressions (neutral, happy, angry, and surprise) - contrasting stereoscopic and monoscopic viewing conditions. The git repository provides the code to analyze the EEG and behavioral data of the study. 

<img src="/visualisation/AvatarMatrix.png" width="260" height="200">

Code based on this research:

ðŸ“œ **Ammara Nasim; Felix Klotzsche; Prof. Dr. Werner Sommer; Prof. Dr. Jochem W. Rieger; Michael Gaebler**

https://tinyurl.com/VRstereofem-Poster-MBBS

ðŸ“œ **Felix Klotzsche; Ammara Nasim; Simon M. Hofmann; Arno Villringer; Vadim V. Nikulin; Werner Sommer; Michael Gaebler**

https://www.cbs.mpg.de/2106779/a27_klotzsche.pdf

https://jov.arvojournals.org/article.aspx?articleid=2792647 

### Output

This code outputs:

Multiclass decoding of all four facial emotions (neutral, happy, angry, surprised) using one vs rest multiclass classification

<img src="/visualisation/ Multiclass decoding_pvalues.jpg" width="360" height="300">

Decoding of all four facial emotions using one vs one classification. This example shows binary decoding classification of Happy vs Angry

<img src="/visualisation/ Happy vs Angry decoding_pvalues.jpg" width="360" height="300">
