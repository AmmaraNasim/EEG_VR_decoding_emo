{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd3de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "%gui qt5\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d653ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as op\n",
    "\n",
    "from os.path import exists\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "from mne.decoding import SlidingEstimator\n",
    "from sklearn.utils import shuffle\n",
    "import sklearn\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import batch_average as ba\n",
    "\n",
    "# to save trained models and load using for making predictions\n",
    "from joblib import dump, load\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f853e",
   "metadata": {},
   "source": [
    "### Importing preprocessed data - epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "overwrite = True\n",
    "eeg_path = r'D:\\Master_thesis\\Pre-processed_data'\n",
    "data_path_out = r'D:\\Master_thesis\\Figures\\Stereovsmono'\n",
    "data_path_out_scores = r'D:\\Master_thesis\\Scores\\Stereovsmono'\n",
    "data_path_out_predictions = r'D:\\Master_thesis\\Predictions\\Stereovsmono'\n",
    "path_model_output = r'D:\\Master_thesis\\TrainedModels\\Stereovsmono'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bca68d",
   "metadata": {},
   "source": [
    "# Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73847a50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1_ratio = [0.1, 0.01]\n",
    "C = [0.5]\n",
    "scoring = 'accuracy'\n",
    "\n",
    "retrain_model = True\n",
    "\n",
    "\n",
    "n_split= np.arange(1, 6, dtype=int)\n",
    "# n_split= [1]\n",
    "subjects = ['VR2FEM_S01', 'VR2FEM_S02', 'VR2FEM_S03', 'VR2FEM_S04', 'VR2FEM_S05','VR2FEM_S06','VR2FEM_S07',\n",
    "        'VR2FEM_S08','VR2FEM_S10','VR2FEM_S11','VR2FEM_S12', 'VR2FEM_S14', \n",
    "        'VR2FEM_S15', 'VR2FEM_S17', 'VR2FEM_S18', 'VR2FEM_S21']\n",
    "\n",
    "# subjects = ['VR2FEM_S18']\n",
    "\n",
    "viewing_conditions = [\"stereo/happy\", \"stereo/neutral\", \"stereo/angry\", \"stereo/surprised\"], \\\n",
    "                    [\"mono/happy\", \"mono/neutral\", \"mono/angry\", \"mono/surprised\"]\n",
    "# os.listdir(eeg_path)\n",
    "\n",
    "monitor_df = pd.DataFrame(columns=[\"subject_id\", \"viewing_condition\", \"viewing_type\", \"l1_ratio\", \"cs\", \"split\",\n",
    "                                  \"scores\", \"predictions\", \"y_test\"])\n",
    "\n",
    "for subject in subjects:\n",
    "# directory management\n",
    "    model_dir_save= os.path.join(path_model_output, subject)\n",
    "    if not op.exists(model_dir_save):\n",
    "        os.makedirs(model_dir_save)\n",
    "    \n",
    "    data_path = os.path.join(eeg_path, subject)\n",
    "    #reading preprocessed data -epochs\n",
    "    \n",
    "    epochs_train_list = []\n",
    "    epochs_test_list = []\n",
    "    \n",
    "    \n",
    "    print(\"----------------1.1 Start Reading of Data ----------------\")\n",
    "    for items in n_split:\n",
    "        \n",
    "        epochs_train_name = os.path.join(f\"{data_path}\\{subject}-preprocessed_train_{items}-epo.fif\")\n",
    "        epochs_test_name = os.path.join(f\"{data_path}\\{subject}-preprocessed_test_{items}-epo.fif\")\n",
    "        epochs_train = mne.read_epochs(epochs_train_name, preload=True)\n",
    "        epochs_test = mne.read_epochs(epochs_test_name, preload=True)\n",
    "        epochs_train_list.append(epochs_train)\n",
    "        epochs_test_list.append(epochs_test)\n",
    "        \n",
    "    print(\"----------------1.1 End Reading of Data ----------------\")\n",
    "    \n",
    "    for view in viewing_conditions:\n",
    "        print(\"----------------1.2 Pipeline Construction ----------------\")\n",
    "        print(f\"Viewing Condition: {view}\")\n",
    "        viewing_type = view[0].split(\"/\")[0]\n",
    "        \n",
    "        scores_avg_all = []\n",
    "        predictions_all = []\n",
    "        y_test_all = []\n",
    "        \n",
    "        for l1 in l1_ratio:\n",
    "            for cs in C:\n",
    "                clf = make_pipeline(\n",
    "                            StandardScaler(),\n",
    "                            LogisticRegression(multi_class='ovr', max_iter = 4000, solver='saga',\n",
    "                                               penalty='elasticnet', l1_ratio=l1, C=cs)\n",
    "                    )\n",
    "                print(\"----------------Start Split ----------------\")\n",
    "\n",
    "                scores_all = []\n",
    "                y_test_split=[]\n",
    "                predictions_split=[]\n",
    "                \n",
    "                for split in n_split:\n",
    "                    model_output_file_name = f\"{model_dir_save}\\{subject}_LogisticsRegression_\\\n",
    "                                                L1_{l1}_C_{cs}_split_{split}.joblib\"\n",
    "\n",
    "                    model_exists = exists(model_output_file_name)\n",
    "                    print(f\"model_exists {model_exists} for {model_output_file_name}\")\n",
    "                    if model_exists:\n",
    "                        print(f\"Trained Model exists file name: {model_output_file_name}\")\n",
    "                    if model_exists is False or retrain_model is True:\n",
    "\n",
    "                        print(f\"Train/test split: {items}\")\n",
    "                        print(\"Running reg. ratio for: \",l1)\n",
    "                        print(\"Running reg. strength for: \",cs)\n",
    "\n",
    "                        print(f\"Subject: {subject} Number of test/train split: {split} for L1 ratio : {l1} \\\n",
    "                                and strength: {cs}\")\n",
    "                        n = split-1\n",
    "\n",
    "                        epochs_train = epochs_train_list[n][view].copy().crop(0.14, 0.20)\n",
    "                        events_train = epochs_train_list[n][view].events[:,2]\n",
    "                        x_train = np.mean(epochs_train, axis = 2)\n",
    "                        y_train = [int(str(yy)[-1]) for yy in events_train]\n",
    "\n",
    "                        epochs_test = epochs_test_list[n][view].copy().crop(0.14, 0.20)\n",
    "                        events_test = epochs_test_list[n][view].events[:,2]\n",
    "                        x_test = np.mean(epochs_test, axis = 2)\n",
    "                        y_test = [int(str(yy)[-1]) for yy in events_test] \n",
    "\n",
    "                        clf.fit(x_train, y_train)\n",
    "                        predictions = clf.predict(x_test)\n",
    "                        scores = clf.score(x_test, y_test)\n",
    "                        print(\"---scores---\", scores)\n",
    "                        scores_all.append(scores)\n",
    "                        predictions_split.append(predictions)\n",
    "                        y_test_split.append(y_test)\n",
    "                        print(y_test_split)\n",
    "                        print(f\"----------------Saving Trained Models to files: {model_output_file_name} ----------------\")\n",
    "\n",
    "                        monitor_df = monitor_df.append({\n",
    "                                \"subject_id\": subject,\n",
    "                                \"viewing_condition\": view,\n",
    "                                \"viewing_type\": viewing_type,\n",
    "                                \"l1_ratio\": l1,\n",
    "                                \"cs\": cs,\n",
    "                                \"split\": split,\n",
    "                                \"scores\": scores,\n",
    "                                \"predictions\": predictions,\n",
    "                                \"y_test\": y_test,\n",
    "                            },\n",
    "                            ignore_index=True)\n",
    "    \n",
    "                if len(scores_all) > 0:\n",
    "                    print(\"----------------Ending Split ----------------\")\n",
    "                    scores_CV = np.vstack(scores_all)\n",
    "                    print(scores_CV.shape)\n",
    "                    scores_avg = np.mean(scores_CV, axis=0)\n",
    "                    print(\"scores_avg.shape\", scores_avg.shape)\n",
    "                    scores_avg_all.append(scores_avg)\n",
    "                    y_test_all.append(y_test_split)\n",
    "                    predictions_all.append(predictions_split)\n",
    "\n",
    "\n",
    "\n",
    "        scores_avg_best = max(scores_avg_all)\n",
    "        print(scores_avg_all)\n",
    "        print(\"scores_all\", scores_avg_best)\n",
    "        result = np.where(scores_avg_all == max(scores_avg_all))\n",
    "        print(\"result\", result[0])\n",
    "        best_model_index = int(result[0])\n",
    "        print(\"best_model_index\", type(best_model_index))\n",
    "        print(\"predictions_all\", predictions_all[0][best_model_index])\n",
    "        print(\"y_test_all\", y_test_all[0][best_model_index])\n",
    "        \n",
    "        predictions_best_model = predictions_all[0][best_model_index]\n",
    "        y_test_best_model = y_test_all[0][best_model_index]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365f009a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_models_df = monitor_df.groupby(['subject_id', 'viewing_type', 'l1_ratio', 'cs']).agg(\n",
    "    scores=pd.NamedAgg(column='scores', aggfunc='mean'),\n",
    "    predictions=pd.NamedAgg(column='predictions', aggfunc=np.hstack),\n",
    "    y_test=pd.NamedAgg(column='y_test', aggfunc=np.hstack)\n",
    ").reset_index()\n",
    "subject_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eceede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- best model logic per subject and viewing type---------------------------\n",
    "\n",
    "best_model_index = subject_models_df.\\\n",
    "    groupby(['subject_id', 'viewing_type'])['scores'].transform(max) == subject_models_df['scores']\n",
    "\n",
    "# print(best_model_index)\n",
    "subject_models_df[best_model_index]\n",
    "\n",
    "# subject_models_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ff99e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------- combine results for all subjects based on viewing type---------------------------\n",
    "\n",
    "combine_model_output = subject_models_df[best_model_index].groupby(['viewing_type']).agg(\n",
    "    scores=pd.NamedAgg(column='scores', aggfunc='mean'),\n",
    "    predictions=pd.NamedAgg(column='predictions', aggfunc=np.hstack),\n",
    "    y_test=pd.NamedAgg(column='y_test', aggfunc=np.hstack)\n",
    ").reset_index()\n",
    "combine_model_output\n",
    "\n",
    "# subject_models_df[best_model_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c29de",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewing_types = combine_model_output[\"viewing_type\"].unique()\n",
    "# print(viewing_types)\n",
    "for view in viewing_types:\n",
    "    print(\"Running confusion matrix for: \", view)\n",
    "    final_cm_df = combine_model_output[combine_model_output.viewing_type == view]\n",
    "    \n",
    "    \n",
    "    y_test = final_cm_df['y_test'].tolist()[0]\n",
    "    predictions = final_cm_df['predictions'].tolist()[0]\n",
    "    cm = metrics.confusion_matrix(y_test, predictions)\n",
    "    print(cm)\n",
    "\n",
    "    final_cm_score = round(final_cm_df['scores'].tolist()[0],3)\n",
    "    plt.figure(figsize=(9,9))\n",
    "    sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');\n",
    "    all_sample_title = f'Accuracy Score for viewing type {view}: {final_cm_score}'\n",
    "    plt.title(all_sample_title, size = 15);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
